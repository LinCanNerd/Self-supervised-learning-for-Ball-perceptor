{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image_dirs = [\"data/naodevils/images\", \"data/bhuman\"]\n",
    "data_dirs = [\"data/naodevils/spqr_autolabel_and_manual_train_patchified.csv\", \"data/bhuman/b-alls-2019_train_patchified.csv\"]\n",
    "val_dirs = [\"data/naodevils/spqr_manual_val_patchified.csv\", \"data/bhuman/b-alls-2019_val_patchified.csv\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BallDataset(Dataset):\n",
    "    def __init__(self, data_dirs, images_dirs, transform=None):\n",
    "        self.data_dirs = data_dirs\n",
    "        self.images_dirs = images_dirs\n",
    "        self.transform = transform\n",
    "        \n",
    "        dfs = []\n",
    "        for data_dir, images_dir in zip(data_dirs, images_dirs):\n",
    "            df = pd.read_csv(data_dir)\n",
    "            df['images_dir'] = images_dir  \n",
    "            dfs.append(df)\n",
    "        self.df = pd.concat(dfs, ignore_index=True)\n",
    "        \n",
    "        if self.transform:\n",
    "            self.dimension = transform.transforms[0].size[0]\n",
    "        else:\n",
    "            self.dimension = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        img_dir = row[\"images_dir\"]\n",
    "        \n",
    "        img = Image.open(os.path.join(img_dir, row[\"image\"])).convert(\"L\")\n",
    "        \n",
    "        patch_dim = (row[\"patch_x\"], row[\"patch_y\"], row[\"patch_x\"] + row[\"patch_size\"], row[\"patch_y\"] + row[\"patch_size\"])\n",
    "        patch = img.crop(patch_dim)\n",
    "        \n",
    "        resize_ratio = self.dimension / row[\"patch_size\"] if self.dimension else 1\n",
    "        \n",
    "        if row[\"patch_contains_ball\"]:\n",
    "            x = (row[\"center_x\"] - row[\"patch_x\"]) * resize_ratio\n",
    "            y = (row[\"center_y\"] - row[\"patch_y\"]) * resize_ratio\n",
    "            r = row[\"radius\"] * resize_ratio\n",
    "        else:\n",
    "            x, y, r = float('nan'), float('nan'), float('nan')\n",
    "        \n",
    "        if self.transform:\n",
    "            patch = self.transform(patch)\n",
    "        \n",
    "        patch = patch * 255.0\n",
    "\n",
    "        #assert pd.isna(x) or x <= 32, (row[\"patch_x\"], row[\"patch_size\"], row[\"center_x\"], x)\n",
    "\n",
    "        return patch, row[\"patch_contains_ball\"], {\"x\": x, \"y\": y, \"r\": r}\n",
    "    \n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x300 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib.patches import Circle\n",
    "\n",
    "dataset = BallDataset(data_dirs, image_dirs, transform)\n",
    "validation = BallDataset(val_dirs, image_dirs, transform)\n",
    "\n",
    "batch_size = 256\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "sample = next(iter(data_loader))\n",
    "\n",
    "num_images = 5\n",
    "\n",
    "ig, axes = plt.subplots(2, num_images, figsize=(num_images * 3, 3))\n",
    "\n",
    "for i in range(num_images):\n",
    "    axes[0, i].imshow(sample[0][i][0], cmap='gray')\n",
    "    if sample[1][i]:\n",
    "        c = Circle((sample[2][\"x\"][i].item(),sample[2][\"y\"][i].item()), sample[2][\"r\"][i].item())\n",
    "        c.set_facecolor((1.0,0.,0.,0.4))\n",
    "        axes[0, i].add_patch(c)\n",
    "    axes[0, i].axis('off')\n",
    "    axes[1, i].text(0.5, 0, f'Contains Ball: {sample[1][i]}', horizontalalignment='center')  # Add text under each image\n",
    "    axes[1, i].axis('off')  # Turn off axis for text\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, use_residual=False):\n",
    "        super(DepthwiseSeparableConv, self).__init__()\n",
    "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=stride, padding=1, groups=in_channels, bias=False)\n",
    "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.use_residual = use_residual\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.depthwise(x)\n",
    "        z = self.pointwise(z)\n",
    "        z = self.bn(z)\n",
    "        z = self.relu(z) + x if self.use_residual else self.relu(z)\n",
    "        return z\n",
    "\n",
    "\n",
    "class BallPerceptor(nn.Module):\n",
    "    def __init__(self, backbone_size = 9):\n",
    "        super(BallPerceptor, self).__init__()\n",
    "        \n",
    "        self.conv1 = DepthwiseSeparableConv(1, 8, stride=1)\n",
    "        self.conv2 = DepthwiseSeparableConv(8, 16, stride=1)\n",
    "        self.conv3 = DepthwiseSeparableConv(16, 32, stride=1)\n",
    "        \n",
    "        backbone_layers = []\n",
    "        for _ in range(backbone_size):\n",
    "            backbone_layers.append(DepthwiseSeparableConv(32, 32, stride=1,use_residual=True))\n",
    "\n",
    "        self.backbone = nn.Sequential(*backbone_layers)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    " \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(32 * 4 * 4, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "        )\n",
    "        self.segmenter = nn.Sequential(\n",
    "            nn.Linear(32 * 4 * 4, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 3), # x, y, r\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.conv1(x))\n",
    "        x = self.pool(self.conv2(x))\n",
    "        x = self.pool(self.conv3(x))\n",
    "        \n",
    "        x = self.backbone(x)\n",
    "\n",
    "        feature_map = x.view(-1, 32 * 4 * 4)\n",
    "\n",
    "        classification = self.classifier(feature_map) if self.training else torch.sigmoid(self.classifier(feature_map))\n",
    "        segmentation = self.segmenter(feature_map)\n",
    "        return classification, segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.modules.loss import _Loss\n",
    "\n",
    "class Circular_DIoU(_Loss):\n",
    "    def __init__(self, size_average=None, reduce=None, reduction: str = 'mean') -> None:\n",
    "        super().__init__(size_average, reduce, reduction)\n",
    "\n",
    "    def intersectionArea(self, X1, Y1, R1, X2, Y2, R2):\n",
    "        d = torch.sqrt(((X2 - X1) * (X2 - X1)) + ((Y2 - Y1) * (Y2 - Y1)))\n",
    "\n",
    "        # Case 1: no overlap \n",
    "        mask_1 = d > R1 + R2\n",
    "\n",
    "        # Case 2: One circle is fully within the other\n",
    "        mask_2 = ( d <= (R1 - R2) ) & ( R1 >= R2 )\n",
    "        mask_3 = ( d <= (R2 - R1) ) & ( R2 > R1 )\n",
    "\n",
    "        # Case 3: Partial overlap\n",
    "        mask_else = ~(mask_1 | mask_2 | mask_3)\n",
    "\n",
    "        intersection_area = torch.zeros_like(d, dtype=torch.float32)\n",
    "\n",
    "        intersection_area[mask_2] = torch.pi * R2[mask_2] * R2[mask_2]\n",
    "        intersection_area[mask_3] = torch.pi * R1[mask_3] * R1[mask_3]\n",
    "\n",
    "        alpha = torch.acos(((R1[mask_else] * R1[mask_else]) + (d[mask_else] * d[mask_else]) - (R2[mask_else] * R2[mask_else])) / (2 * R1[mask_else] * d[mask_else])) * 2\n",
    "        beta = torch.acos(((R2[mask_else] * R2[mask_else]) + (d[mask_else] * d[mask_else]) - (R1[mask_else] * R1[mask_else])) / (2 * R2[mask_else] * d[mask_else])) * 2\n",
    "        \n",
    "        a1 = (0.5 * beta * R2[mask_else] * R2[mask_else] ) - (0.5 * R2[mask_else] * R2[mask_else] * torch.sin(beta))\n",
    "        a2 = (0.5 * alpha * R1[mask_else] * R1[mask_else]) - (0.5 * R1[mask_else] * R1[mask_else] * torch.sin(alpha))\n",
    "\n",
    "        intersection_area[mask_else] = a1 + a2\n",
    "\n",
    "        return intersection_area\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        a1 = torch.pi * torch.pow(input[:,2], 2)\n",
    "        a2 = torch.pi * torch.pow(target[:,2], 2)\n",
    "\n",
    "        a1inta2 = self.intersectionArea(input[:,0], input[:,1], input[:,2], target[:,0], target[:,1], target[:,2])\n",
    "\n",
    "        a1una2 = a1 + a2 - a1inta2\n",
    "\n",
    "        center_diff = input[:,:2] - target[:,:2]\n",
    "\n",
    "        center_dist = torch.norm(center_diff, dim=-1)\n",
    "        D = torch.pow(center_dist, 2)/torch.pow(center_dist + input[:,2] + target[:,2], 2)\n",
    "\n",
    "        DIoU = 1 - a1inta2/a1una2 + D\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return DIoU.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return DIoU.sum()\n",
    "        else:\n",
    "            return DIoU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersectionArea(X1, Y1, R1, X2, Y2, R2):\n",
    "        d = torch.sqrt(((X2 - X1) * (X2 - X1)) + ((Y2 - Y1) * (Y2 - Y1)))\n",
    "\n",
    "        # Case 1: no overlap \n",
    "        mask_1 = d > R1 + R2\n",
    "\n",
    "        # Case 2: One circle is fully within the other\n",
    "        mask_2 = ( d <= (R1 - R2) ) & ( R1 >= R2 )\n",
    "        mask_3 = ( d <= (R2 - R1) ) & ( R2 > R1 )\n",
    "\n",
    "        # Case 3: Partial overlap\n",
    "        mask_else = ~(mask_1 | mask_2 | mask_3)\n",
    "\n",
    "        intersection_area = torch.zeros_like(d, dtype=torch.float32)\n",
    "\n",
    "        intersection_area[mask_2] = torch.pi * R2[mask_2] * R2[mask_2]\n",
    "        intersection_area[mask_3] = torch.pi * R1[mask_3] * R1[mask_3]\n",
    "\n",
    "        alpha = torch.acos(((R1[mask_else] * R1[mask_else]) + (d[mask_else] * d[mask_else]) - (R2[mask_else] * R2[mask_else])) / (2 * R1[mask_else] * d[mask_else])) * 2\n",
    "        beta = torch.acos(((R2[mask_else] * R2[mask_else]) + (d[mask_else] * d[mask_else]) - (R1[mask_else] * R1[mask_else])) / (2 * R2[mask_else] * d[mask_else])) * 2\n",
    "\n",
    "        a1 = (0.5 * beta * R2[mask_else] * R2[mask_else] ) - (0.5 * R2[mask_else] * R2[mask_else] * torch.sin(beta))\n",
    "        a2 = (0.5 * alpha * R1[mask_else] * R1[mask_else]) - (0.5 * R1[mask_else] * R1[mask_else] * torch.sin(alpha))\n",
    "\n",
    "        intersection_area[mask_else] = a1 + a2\n",
    "\n",
    "        return intersection_area\n",
    "\n",
    "def IoU(input, target):\n",
    "        a1 = torch.pi * torch.pow(input[:,2], 2)\n",
    "        a2 = torch.pi * torch.pow(target[:,2], 2)\n",
    "        a1inta2 = intersectionArea(input[:,0], input[:,1], input[:,2], target[:,0], target[:,1], target[:,2])\n",
    "        iou = a1inta2/ (a1 + a2 - a1inta2)\n",
    "        center_diff = input[:,:2] - target[:,:2]\n",
    "        center_dist = torch.norm(center_diff, dim=-1)\n",
    "        D = torch.pow(center_dist, 2)/torch.pow(center_dist + input[:,2] + target[:,2], 2)\n",
    "        return iou.mean() , D.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "best_loss = float(\"inf\")\n",
    "#best_loss = 1.8359"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model: nn.Module, detection_weight: float, best_loss: float):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in validation_loader:\n",
    "            images, labels = data[0].to(device), data[1].float().to(device)\n",
    "            segmentation_gt = torch.stack((data[2][\"x\"], data[2][\"y\"], data[2][\"r\"]), dim=1).to(device).to(torch.float)\n",
    "            classification_mask = labels == 1.\n",
    "\n",
    "            classification, segmentation = model(images)\n",
    "\n",
    "            val_loss += classification_criterion(classification, labels.unsqueeze(1))                    \n",
    "            if classification_mask.any():\n",
    "                val_loss += detection_weight * segmentation_criterion(segmentation[classification_mask], segmentation_gt[classification_mask])\n",
    "\n",
    "            predicted = torch.round(classification)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels.unsqueeze(1)).sum().item()\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    model.train()\n",
    "    accuracy = 100 * correct / total\n",
    "    val_loss /= total / batch_size\n",
    "\n",
    "    # Compute F1-score\n",
    "    f1 = f1_score(all_labels, all_predictions, average='binary')\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        torch.save(model.state_dict(), PATH)\n",
    "\n",
    "    model.train()\n",
    "    return val_loss, accuracy, f1*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from datetime import datetime\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model: nn.Module, detection_weight: float, best_loss: float, path:str):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in validation_loader:\n",
    "            images, labels = data[0].to(device), data[1].float().to(device)\n",
    "            segmentation_gt = torch.stack((data[2][\"x\"], data[2][\"y\"], data[2][\"r\"]), dim=1).to(device).to(torch.float)\n",
    "            classification_mask = labels == 1.\n",
    "\n",
    "            classification, segmentation = model(images)\n",
    "\n",
    "            val_loss += classification_criterion(classification, labels.unsqueeze(1))                    \n",
    "            if classification_mask.any():\n",
    "                val_loss += detection_weight * segmentation_criterion(segmentation[classification_mask], segmentation_gt[classification_mask])\n",
    "                iou, d = IoU(segmentation[classification_mask], segmentation_gt[classification_mask])\n",
    "            predicted = torch.round(classification)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels.unsqueeze(1)).sum().item()\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    model.train()\n",
    "    accuracy = 100 * correct / total\n",
    "    val_loss /= total / batch_size\n",
    "\n",
    "    # Compute F1-score\n",
    "    f1 = f1_score(all_labels, all_predictions, average='binary')\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        torch.save(model.state_dict(), path)\n",
    "\n",
    "    model.train()\n",
    "    return val_loss, accuracy, f1*100, iou, d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.multiprocessing\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "dataset = BallDataset(data_dirs, image_dirs, transform)\n",
    "validation = BallDataset(val_dirs, image_dirs, transform)\n",
    "\n",
    "batch_size = 256\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "validation_loader = DataLoader(validation, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights(model, weights_path=None):\n",
    "    if weights_path is None:\n",
    "        return model\n",
    "    weights = torch.load(weights_path)\n",
    "    model.load_state_dict(weights, strict=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "p_weight = torch.tensor([0.6]).to(device)\n",
    "classification_criterion = nn.BCEWithLogitsLoss(pos_weight=p_weight)\n",
    "segmentation_criterion = Circular_DIoU()\n",
    "\n",
    "num_epochs = 5\n",
    "iterations = 5 #how many iterations in total\n",
    "log_rate = 100\n",
    "eval_rate = 300\n",
    "\n",
    "\n",
    "##### CHANGE HERE TO TEST DIFFERENT MODELS##########\n",
    "\n",
    "path = \"results/###\" #path to save model\n",
    "best_loss_file = path.replace('.pt', '.txt') #path to save results\n",
    "weights_path = '' #path for loading weights\n",
    "\n",
    "############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder \"results\" already exists.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "folder_name = \"results\"\n",
    "\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "    print(f'Folder \"{folder_name}\" created.')\n",
    "else:\n",
    "    print(f'Folder \"{folder_name}\" already exists.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_weight = 5\n",
    "best_loss = float(\"inf\")\n",
    "tot_loss = 0\n",
    "logs = [\"\" for i in range(iterations)]\n",
    "\n",
    "for it in range(iterations):\n",
    "    best_loss = float(\"inf\")\n",
    "    model = BallPerceptor()\n",
    "    model = load_weights(model,weights_path)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.0001)\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, data in tqdm.tqdm(enumerate(data_loader, 0), total=len(dataset)//batch_size):\n",
    "            inputs, labels = data[0].to(device), data[1].float().to(device)\n",
    "            segmentation_gt = torch.stack((data[2][\"x\"], data[2][\"y\"], data[2][\"r\"]), dim=1).to(device).to(torch.float)\n",
    "\n",
    "            classification_mask = labels == 1.\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            classification, segmentation = model(inputs)\n",
    "            loss = classification_criterion(classification, labels.unsqueeze(1))\n",
    "            if classification_mask.any():\n",
    "                loss = detection_weight*segmentation_criterion(segmentation[classification_mask], segmentation_gt[classification_mask]) + loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if (i+1) % log_rate == 0: \n",
    "                print('[Epoch %d, Batch %5d] Training loss: %.5f' %\n",
    "                    (epoch + 1, i + 1, running_loss / log_rate))\n",
    "                running_loss = 0.0\n",
    "        \n",
    "            if (i + 1) % eval_rate == 0:\n",
    "                val_loss, accuracy, f1, iou, d = validate(model, detection_weight, best_loss, path)\n",
    "                print('Epoch: %d --- Accuracy: %f --- F1: %f --- Loss: %f --- IoU: %f --- D: %f' % (epoch + 1, accuracy, f1, val_loss, iou, d))\n",
    "\n",
    "                if val_loss < best_loss:\n",
    "                    best_loss = val_loss\n",
    "                    log = 'Epoch: %d --- Accuracy: %f --- F1: %f  --- Loss: %f --- IoU: %f --- D: %f' % (epoch + 1, accuracy, f1, val_loss, iou, d)\n",
    "                    logs[it] = log\n",
    "\n",
    "        val_loss, accuracy, f1, iou, d = validate(model, detection_weight, best_loss, path)\n",
    "        print('Epoch: %d --- Accuracy: %f --- F1: %f --- Loss: %f --- IoU: %f --- D: %f' % (epoch + 1, accuracy, f1, val_loss, iou, d))\n",
    "    tot_loss += best_loss\n",
    "\n",
    "avg_loss = tot_loss / iterations\n",
    "with open(best_loss_file, 'a') as f:\n",
    "    for l in logs:\n",
    "        f.write(l + '\\n')\n",
    "    f.write(f'Average loss is: {avg_loss}\\n')\n",
    "\n",
    "print('Finished Training')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "FDSvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
