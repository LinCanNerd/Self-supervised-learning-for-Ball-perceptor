{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image_dirs = [\"data/naodevils/images\", \"data/bhuman\"]\n",
    "data_dirs = [\"data/naodevils/spqr_autolabel_and_manual_train_patchified.csv\", \"data/bhuman/b-alls-2019_train_patchified.csv\"]\n",
    "val_dirs = [\"data/naodevils/spqr_manual_val_patchified.csv\", \"data/bhuman/b-alls-2019_val_patchified.csv\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BallDataset(Dataset):\n",
    "    def __init__(self, data_dirs, images_dirs, transform=None):\n",
    "        self.data_dirs = data_dirs\n",
    "        self.images_dirs = images_dirs\n",
    "        self.transform = transform\n",
    "        \n",
    "        dfs = []\n",
    "        for data_dir, images_dir in zip(data_dirs, images_dirs):\n",
    "            df = pd.read_csv(data_dir)\n",
    "            df['images_dir'] = images_dir  \n",
    "            dfs.append(df)\n",
    "        self.df = pd.concat(dfs, ignore_index=True)\n",
    "        \n",
    "        if self.transform:\n",
    "            self.dimension = transform.transforms[0].size[0]\n",
    "        else:\n",
    "            self.dimension = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        \n",
    "        img_dir = row[\"images_dir\"]\n",
    "        \n",
    "        img = Image.open(os.path.join(img_dir, row[\"image\"])).convert(\"L\")\n",
    "        \n",
    "        patch_dim = (row[\"patch_x\"], row[\"patch_y\"], row[\"patch_x\"] + row[\"patch_size\"], row[\"patch_y\"] + row[\"patch_size\"])\n",
    "        patch = img.crop(patch_dim)\n",
    "        \n",
    "        resize_ratio = self.dimension / row[\"patch_size\"] if self.dimension else 1\n",
    "        \n",
    "        if row[\"patch_contains_ball\"]:\n",
    "            x = (row[\"center_x\"] - row[\"patch_x\"]) * resize_ratio\n",
    "            y = (row[\"center_y\"] - row[\"patch_y\"]) * resize_ratio\n",
    "            r = row[\"radius\"] * resize_ratio\n",
    "        else:\n",
    "            x, y, r = float('nan'), float('nan'), float('nan')\n",
    "        \n",
    "        if self.transform:\n",
    "            patch = self.transform(patch)\n",
    "        \n",
    "        patch = patch * 255.0\n",
    "\n",
    "        #assert pd.isna(x) or x <= 32, (row[\"patch_x\"], row[\"patch_size\"], row[\"center_x\"], x)\n",
    "\n",
    "        return patch, row[\"patch_contains_ball\"], {\"x\": x, \"y\": y, \"r\": r}\n",
    "    \n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Circle\n",
    "\n",
    "dataset = BallDataset(data_dirs, image_dirs, transform)\n",
    "validation = BallDataset(val_dirs, image_dirs, transform)\n",
    "\n",
    "batch_size = 256\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "validation_loader = DataLoader(validation, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "sample = next(iter(data_loader))\n",
    "\n",
    "num_images = 5\n",
    "\n",
    "ig, axes = plt.subplots(2, num_images, figsize=(num_images * 3, 3))\n",
    "\n",
    "for i in range(num_images):\n",
    "    axes[0, i].imshow(sample[0][i][0], cmap='gray')\n",
    "    if sample[1][i]:\n",
    "        c = Circle((sample[2][\"x\"][i].item(),sample[2][\"y\"][i].item()), sample[2][\"r\"][i].item())\n",
    "        c.set_facecolor((1.0,0.,0.,0.4))\n",
    "        axes[0, i].add_patch(c)\n",
    "    axes[0, i].axis('off')\n",
    "    axes[1, i].text(0.5, 0, f'Contains Ball: {sample[1][i]}', horizontalalignment='center')  # Add text under each image\n",
    "    axes[1, i].axis('off')  # Turn off axis for text\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, use_residual=False):\n",
    "        super(DepthwiseSeparableConv, self).__init__()\n",
    "        self.depthwise = nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=stride, padding=1, groups=in_channels, bias=False)\n",
    "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.use_residual = use_residual\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.depthwise(x)\n",
    "        z = self.pointwise(z)\n",
    "        z = self.bn(z)\n",
    "        z = self.relu(z) + x if self.use_residual else self.relu(z)\n",
    "        return z\n",
    "\n",
    "\n",
    "class BallPerceptor(nn.Module):\n",
    "    def __init__(self, backbone_size = 9):\n",
    "        super(BallPerceptor, self).__init__()\n",
    "        \n",
    "        self.conv1 = DepthwiseSeparableConv(1, 8, stride=1)\n",
    "        self.conv2 = DepthwiseSeparableConv(8, 16, stride=1)\n",
    "        self.conv3 = DepthwiseSeparableConv(16, 32, stride=1)\n",
    "        \n",
    "        backbone_layers = []\n",
    "        for _ in range(backbone_size):\n",
    "            backbone_layers.append(DepthwiseSeparableConv(32, 32, stride=1,use_residual=True))\n",
    "\n",
    "        self.backbone = nn.Sequential(*backbone_layers)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    " \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(32 * 4 * 4, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "        )\n",
    "        self.segmenter = nn.Sequential(\n",
    "            nn.Linear(32 * 4 * 4, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 3), # x, y, r\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.conv1(x))\n",
    "        x = self.pool(self.conv2(x))\n",
    "        x = self.pool(self.conv3(x))\n",
    "        \n",
    "        x = self.backbone(x)\n",
    "\n",
    "        feature_map = x.view(-1, 32 * 4 * 4)\n",
    "\n",
    "        classification = self.classifier(feature_map) if self.training else torch.sigmoid(self.classifier(feature_map))\n",
    "        segmentation = self.segmenter(feature_map)\n",
    "        return classification, segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.modules.loss import _Loss\n",
    "\n",
    "class Circular_DIoU(_Loss):\n",
    "    def __init__(self, size_average=None, reduce=None, reduction: str = 'mean') -> None:\n",
    "        super().__init__(size_average, reduce, reduction)\n",
    "\n",
    "    def intersectionArea(self, X1, Y1, R1, X2, Y2, R2):\n",
    "        d = torch.sqrt(((X2 - X1) * (X2 - X1)) + ((Y2 - Y1) * (Y2 - Y1)))\n",
    "\n",
    "        # Case 1: no overlap \n",
    "        mask_1 = d > R1 + R2\n",
    "\n",
    "        # Case 2: One circle is fully within the other\n",
    "        mask_2 = ( d <= (R1 - R2) ) & ( R1 >= R2 )\n",
    "        mask_3 = ( d <= (R2 - R1) ) & ( R2 > R1 )\n",
    "\n",
    "        # Case 3: Partial overlap\n",
    "        mask_else = ~(mask_1 | mask_2 | mask_3)\n",
    "\n",
    "        intersection_area = torch.zeros_like(d, dtype=torch.float32)\n",
    "\n",
    "        intersection_area[mask_2] = torch.pi * R2[mask_2] * R2[mask_2]\n",
    "        intersection_area[mask_3] = torch.pi * R1[mask_3] * R1[mask_3]\n",
    "\n",
    "        alpha = torch.acos(((R1[mask_else] * R1[mask_else]) + (d[mask_else] * d[mask_else]) - (R2[mask_else] * R2[mask_else])) / (2 * R1[mask_else] * d[mask_else])) * 2\n",
    "        beta = torch.acos(((R2[mask_else] * R2[mask_else]) + (d[mask_else] * d[mask_else]) - (R1[mask_else] * R1[mask_else])) / (2 * R2[mask_else] * d[mask_else])) * 2\n",
    "        \n",
    "        a1 = (0.5 * beta * R2[mask_else] * R2[mask_else] ) - (0.5 * R2[mask_else] * R2[mask_else] * torch.sin(beta))\n",
    "        a2 = (0.5 * alpha * R1[mask_else] * R1[mask_else]) - (0.5 * R1[mask_else] * R1[mask_else] * torch.sin(alpha))\n",
    "\n",
    "        intersection_area[mask_else] = a1 + a2\n",
    "\n",
    "        return intersection_area\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        a1 = torch.pi * torch.pow(input[:,2], 2)\n",
    "        a2 = torch.pi * torch.pow(target[:,2], 2)\n",
    "\n",
    "        a1inta2 = self.intersectionArea(input[:,0], input[:,1], input[:,2], target[:,0], target[:,1], target[:,2])\n",
    "\n",
    "        a1una2 = a1 + a2 - a1inta2\n",
    "\n",
    "        center_diff = input[:,:2] - target[:,:2]\n",
    "\n",
    "        center_dist = torch.norm(center_diff, dim=-1)\n",
    "        D = torch.pow(center_dist, 2)/torch.pow(center_dist + input[:,2] + target[:,2], 2)\n",
    "\n",
    "        DIoU = 1 - a1inta2/a1una2 + D\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return DIoU.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return DIoU.sum()\n",
    "        else:\n",
    "            return DIoU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersectionArea(X1, Y1, R1, X2, Y2, R2):\n",
    "        d = torch.sqrt(((X2 - X1) * (X2 - X1)) + ((Y2 - Y1) * (Y2 - Y1)))\n",
    "\n",
    "        # Case 1: no overlap \n",
    "        mask_1 = d > R1 + R2\n",
    "\n",
    "        # Case 2: One circle is fully within the other\n",
    "        mask_2 = ( d <= (R1 - R2) ) & ( R1 >= R2 )\n",
    "        mask_3 = ( d <= (R2 - R1) ) & ( R2 > R1 )\n",
    "\n",
    "        # Case 3: Partial overlap\n",
    "        mask_else = ~(mask_1 | mask_2 | mask_3)\n",
    "\n",
    "        intersection_area = torch.zeros_like(d, dtype=torch.float32)\n",
    "\n",
    "        intersection_area[mask_2] = torch.pi * R2[mask_2] * R2[mask_2]\n",
    "        intersection_area[mask_3] = torch.pi * R1[mask_3] * R1[mask_3]\n",
    "\n",
    "        alpha = torch.acos(((R1[mask_else] * R1[mask_else]) + (d[mask_else] * d[mask_else]) - (R2[mask_else] * R2[mask_else])) / (2 * R1[mask_else] * d[mask_else])) * 2\n",
    "        beta = torch.acos(((R2[mask_else] * R2[mask_else]) + (d[mask_else] * d[mask_else]) - (R1[mask_else] * R1[mask_else])) / (2 * R2[mask_else] * d[mask_else])) * 2\n",
    "\n",
    "        a1 = (0.5 * beta * R2[mask_else] * R2[mask_else] ) - (0.5 * R2[mask_else] * R2[mask_else] * torch.sin(beta))\n",
    "        a2 = (0.5 * alpha * R1[mask_else] * R1[mask_else]) - (0.5 * R1[mask_else] * R1[mask_else] * torch.sin(alpha))\n",
    "\n",
    "        intersection_area[mask_else] = a1 + a2\n",
    "\n",
    "        return intersection_area\n",
    "\n",
    "def IoU(input, target):\n",
    "        a1 = torch.pi * torch.pow(input[:,2], 2)\n",
    "        a2 = torch.pi * torch.pow(target[:,2], 2)\n",
    "        a1inta2 = intersectionArea(input[:,0], input[:,1], input[:,2], target[:,0], target[:,1], target[:,2])\n",
    "        iou = a1inta2/ (a1 + a2 - a1inta2)\n",
    "        center_diff = input[:,:2] - target[:,:2]\n",
    "        center_dist = torch.norm(center_diff, dim=-1)\n",
    "        D = torch.pow(center_dist, 2)/torch.pow(center_dist + input[:,2] + target[:,2], 2)\n",
    "        return iou.mean() , D.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# No freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "best_loss = float(\"inf\")\n",
    "#best_loss = 1.8359"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model: nn.Module, detection_weight: float, best_loss: float):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in validation_loader:\n",
    "            images, labels = data[0].to(device), data[1].float().to(device)\n",
    "            segmentation_gt = torch.stack((data[2][\"x\"], data[2][\"y\"], data[2][\"r\"]), dim=1).to(device).to(torch.float)\n",
    "            classification_mask = labels == 1.\n",
    "\n",
    "            classification, segmentation = model(images)\n",
    "\n",
    "            val_loss += classification_criterion(classification, labels.unsqueeze(1))                    \n",
    "            if classification_mask.any():\n",
    "                val_loss += detection_weight * segmentation_criterion(segmentation[classification_mask], segmentation_gt[classification_mask])\n",
    "\n",
    "            predicted = torch.round(classification)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels.unsqueeze(1)).sum().item()\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    model.train()\n",
    "    accuracy = 100 * correct / total\n",
    "    val_loss /= total / batch_size\n",
    "\n",
    "    # Compute F1-score\n",
    "    f1 = f1_score(all_labels, all_predictions, average='binary')\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        torch.save(model.state_dict(), PATH)\n",
    "\n",
    "    model.train()\n",
    "    return val_loss, accuracy, f1*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from datetime import datetime\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(model: nn.Module, detection_weight: float, best_loss: float, path:str):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    val_loss = 0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in validation_loader:\n",
    "            images, labels = data[0].to(device), data[1].float().to(device)\n",
    "            segmentation_gt = torch.stack((data[2][\"x\"], data[2][\"y\"], data[2][\"r\"]), dim=1).to(device).to(torch.float)\n",
    "            classification_mask = labels == 1.\n",
    "\n",
    "            classification, segmentation = model(images)\n",
    "\n",
    "            val_loss += classification_criterion(classification, labels.unsqueeze(1))                    \n",
    "            if classification_mask.any():\n",
    "                val_loss += detection_weight * segmentation_criterion(segmentation[classification_mask], segmentation_gt[classification_mask])\n",
    "                iou, d = IoU(segmentation[classification_mask], segmentation_gt[classification_mask])\n",
    "            predicted = torch.round(classification)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels.unsqueeze(1)).sum().item()\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    model.train()\n",
    "    accuracy = 100 * correct / total\n",
    "    val_loss /= total / batch_size\n",
    "\n",
    "    # Compute F1-score\n",
    "    f1 = f1_score(all_labels, all_predictions, average='binary')\n",
    "\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        torch.save(model.state_dict(), path)\n",
    "\n",
    "    model.train()\n",
    "    return val_loss, accuracy, f1*100, iou, d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.multiprocessing\n",
    "torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "dataset = BallDataset(data_dirs, image_dirs, transform)\n",
    "validation = BallDataset(val_dirs, image_dirs, transform)\n",
    "\n",
    "batch_size = 256\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "validation_loader = DataLoader(validation, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_weights(model, weights_path=None):\n",
    "    if weights_path is None:\n",
    "        return model\n",
    "    weights = torch.load(weights_path)\n",
    "    model.load_state_dict(weights, strict=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "p_weight = torch.tensor([0.6]).to(device)\n",
    "classification_criterion = nn.BCEWithLogitsLoss(pos_weight=p_weight)\n",
    "segmentation_criterion = Circular_DIoU()\n",
    "\n",
    "num_epochs = 5\n",
    "iterations = 5 #how many iterations in total\n",
    "log_rate = 100\n",
    "eval_rate = 300\n",
    "\n",
    "\n",
    "##### CHANGE HERE TO TEST DIFFERENT MODELS##########\n",
    "\n",
    "path = \"results/###\" #path to save model\n",
    "best_loss_file = path.replace('.pt', '.txt') #path to save results\n",
    "weights_path = '' #path for loading weights\n",
    "\n",
    "############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder \"results\" already exists.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "folder_name = \"results\"\n",
    "\n",
    "if not os.path.exists(folder_name):\n",
    "    os.makedirs(folder_name)\n",
    "    print(f'Folder \"{folder_name}\" created.')\n",
    "else:\n",
    "    print(f'Folder \"{folder_name}\" already exists.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_weight = 5\n",
    "best_loss = float(\"inf\")\n",
    "tot_loss = 0\n",
    "logs = [\"\" for i in range(iterations)]\n",
    "\n",
    "for it in range(iterations):\n",
    "    best_loss = float(\"inf\")\n",
    "    model = BallPerceptor()\n",
    "    model = load_weights(model,weights_path)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.0001)\n",
    "    model.to(device)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for i, data in tqdm.tqdm(enumerate(data_loader, 0), total=len(dataset)//batch_size):\n",
    "            inputs, labels = data[0].to(device), data[1].float().to(device)\n",
    "            segmentation_gt = torch.stack((data[2][\"x\"], data[2][\"y\"], data[2][\"r\"]), dim=1).to(device).to(torch.float)\n",
    "\n",
    "            classification_mask = labels == 1.\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            classification, segmentation = model(inputs)\n",
    "            loss = classification_criterion(classification, labels.unsqueeze(1))\n",
    "            if classification_mask.any():\n",
    "                loss = detection_weight*segmentation_criterion(segmentation[classification_mask], segmentation_gt[classification_mask]) + loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if (i+1) % log_rate == 0: \n",
    "                print('[Epoch %d, Batch %5d] Training loss: %.5f' %\n",
    "                    (epoch + 1, i + 1, running_loss / log_rate))\n",
    "                running_loss = 0.0\n",
    "        \n",
    "            if (i + 1) % eval_rate == 0:\n",
    "                val_loss, accuracy, f1, iou, d = validate(model, detection_weight, best_loss, path)\n",
    "                print('Epoch: %d --- Accuracy: %f --- F1: %f --- Loss: %f --- IoU: %f --- D: %f' % (epoch + 1, accuracy, f1, val_loss, iou, d))\n",
    "\n",
    "                if val_loss < best_loss:\n",
    "                    best_loss = val_loss\n",
    "                    log = 'Epoch: %d --- Accuracy: %f --- F1: %f  --- Loss: %f --- IoU: %f --- D: %f' % (epoch + 1, accuracy, f1, val_loss, iou, d)\n",
    "                    logs[it] = log\n",
    "\n",
    "        val_loss, accuracy, f1, iou, d = validate(model, detection_weight, best_loss, path)\n",
    "        print('Epoch: %d --- Accuracy: %f --- F1: %f --- Loss: %f --- IoU: %f --- D: %f' % (epoch + 1, accuracy, f1, val_loss, iou, d))\n",
    "    tot_loss += best_loss\n",
    "\n",
    "avg_loss = tot_loss / iterations\n",
    "with open(best_loss_file, 'a') as f:\n",
    "    for l in logs:\n",
    "        f.write(l + '\\n')\n",
    "    f.write(f'Average loss is: {avg_loss}\\n')\n",
    "\n",
    "print('Finished Training')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ProjVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
